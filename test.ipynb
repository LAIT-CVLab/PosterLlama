{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents Aware Layout Generation Example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d92c3c81424a1d9e1f3a6eb8dae1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/poong/.conda/envs/jaepoong/lib/python3.9/site-packages/peft/utils/other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 6,772,101,120 || trainable%: 0.49548037463445316\n",
      "Do not use Q-Former here.\n",
      "Vit_model dino_v2\n",
      "Load 1 training prompts\n",
      "Prompt Example \n",
      "['[INST] <Img><ImageHere></Img> [/INST] ']\n"
     ]
    }
   ],
   "source": [
    "from src.model.minigpt4 import MiniGPT4\n",
    "from src.processor.blip_processors import Blip2ImageTrainProcessor,Blip2ImageEvalProcessor,DinoImageProcessor\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from html_to_ui import get_bbox\n",
    "from generation import *\n",
    "with open(\"log_dir/train_stage2_with_augment_dino_codellama/generated_sample/16/test_numerical.jsonl\", \"r\") as f:\n",
    "    content = [json.loads(line) for line in f]\n",
    "with open(\"data/cgl_dataset/for_posternuwa/html_format_img_instruct_all_mask_and_all_condition/test_numerical.jsonl\", \"r\") as f:\n",
    "    sample = [json.loads(line) for line in f]\n",
    "device = f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\"\n",
    "vit_model_name = \"dino_v2\"\n",
    "llama_model = \"models/codeLlama-7b-hf\"\n",
    "base_model = \"log_dir/train_stage2_with_augment_dino_codellama/checkpoints/checkpoint-16/pytorch_model.bin\"\n",
    "model = MiniGPT4(lora_r=64,low_resource=False,vit_model = vit_model_name,llama_model = llama_model)\n",
    "model.load_state_dict(torch.load(base_model,map_location=\"cpu\"))\n",
    "model = model.to(device)\n",
    "model.device = device\n",
    "model.half()\n",
    "model.eval()\n",
    "\n",
    "image_processor = DinoImageProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional Layout Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = os.listdir(\"asset\")\n",
    "images = []\n",
    "processed_images=[]\n",
    "for path in img_paths:\n",
    "    img = Image.open(os.path.join(\"asset\",path))\n",
    "    images.append(img)\n",
    "    processed_images.append(image_processor(img))\n",
    "processed_images = torch.stack(processed_images,dim=0)\n",
    "uncond_inst = 'I want to generate layout in poster design format.plaese generate the layout html according to the image I provide (in html format):\\n###bbox html: <body> <svg width=\"513\" height=\"750\">  </svg> </body> '\n",
    "inst = [uncond_inst]*len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sort_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m draw_ol \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mImageDraw(drawn_outline)\n\u001b[1;32m     24\u001b[0m draw_f \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mImageDraw(drawn_fill)\n\u001b[0;32m---> 27\u001b[0m sorted_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(clses, drawbbox), key\u001b[38;5;241m=\u001b[39m\u001b[43msort_key\u001b[49m)\n\u001b[1;32m     28\u001b[0m sorted_classes, sorted_bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msorted_pairs)\n\u001b[1;32m     30\u001b[0m clses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sorted_classes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sort_key' is not defined"
     ]
    }
   ],
   "source": [
    "DATASET_COLOR = {\n",
    "    1: '#929F29',   \n",
    "    2: '#1FA39A',  \n",
    "    3: '#987FF2',      \n",
    "    4: '#F56881',    \n",
    "    5: \"#0000FF\"      \n",
    "}\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        samp = model.generate(processed_images, inst,max_new_tokens=512,do_sample=True,temperature=0.6,top_p=0.9)\n",
    "\n",
    "for i in range(len(img_paths)):\n",
    "    bboxes,clses = get_bbox(samp[i])\n",
    "    drawbbox = []\n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x2 += x1\n",
    "        y2 += y1\n",
    "        bbox  = [x1, y1, x2, y2]\n",
    "        drawbbox.append(np.array(bbox))\n",
    "    drawn_outline = images[i].copy()\n",
    "    drawn_fill = images[i].copy()\n",
    "    draw_ol = ImageDraw.ImageDraw(drawn_outline)\n",
    "    draw_f = ImageDraw.ImageDraw(drawn_fill)\n",
    "\n",
    "\n",
    "    sorted_pairs = sorted(zip(clses, drawbbox), key=sort_key)\n",
    "    sorted_classes, sorted_bboxes = zip(*sorted_pairs)\n",
    "\n",
    "    clses = list(sorted_classes)\n",
    "    drawbbox = list(sorted_bboxes)\n",
    "\n",
    "    for b,l in zip(drawbbox,clses):\n",
    "        draw_ol.rectangle([b[0],b[1],b[2],b[3]], outline=DATASET_COLOR[l], width=2)\n",
    "    for b,l in zip(drawbbox,clses):\n",
    "        draw_f.rectangle([b[0],b[1],b[2],b[3]], fill=DATASET_COLOR[l])\n",
    "    drawn_outline = drawn_outline.convert(\"RGBA\")\n",
    "    drawn_fill = drawn_fill.convert(\"RGBA\")\n",
    "\n",
    "    drawn_fill.putalpha(int(256 * 0.4))\n",
    "    img = Image.alpha_composite(drawn_outline, drawn_fill)\n",
    "    display(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaepoong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
